{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # BertModelのロード\n",
    "        self.bert = BertModel.from_pretrained(model_name) \n",
    "        # 線形変換を初期化しておく\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_labels\n",
    "        ) \n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None, \n",
    "        attention_mask=None, \n",
    "        token_type_ids=None, \n",
    "        labels=None\n",
    "    ):\n",
    "        # データを入力しBERTの最終層の出力を得る。\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "        \n",
    "        # [PAD]以外のトークンで隠れ状態の平均をとる\n",
    "        averaged_hidden_state = \\\n",
    "            (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) \\\n",
    "            / attention_mask.sum(1, keepdim=True)\n",
    "        \n",
    "        # 線形変換\n",
    "        scores = self.linear(averaged_hidden_state) \n",
    "        \n",
    "        # 出力の形式を整える。\n",
    "        output = {'logits': scores}\n",
    "\n",
    "        # labelsが入力に含まれていたら、損失を計算し出力する。\n",
    "        if labels is not None: \n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output['loss'] = loss\n",
    "            \n",
    "        # 属性でアクセスできるようにする。\n",
    "        output = type('bert_output', (object,), output) \n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizerとモデルのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "    MODEL_NAME, num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "text_list = [\n",
    "    '今日の仕事はうまくいったが、体調があまり良くない。',\n",
    "    '昨日は楽しかった。'\n",
    "]\n",
    "\n",
    "labels_list = [\n",
    "    [1, 1],\n",
    "    [0, 1]\n",
    "]\n",
    "\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding='longest',  \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v for k, v in encoding.items() }\n",
    "labels = torch.tensor(labels_list)\n",
    "\n",
    "# BERTへデータを入力し分類スコアを得る。\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits\n",
    "\n",
    "# スコアが正ならば、そのカテゴリーを選択する。\n",
    "labels_predicted = ( scores > 0 ).int()\n",
    "\n",
    "# 精度の計算\n",
    "num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "accuracy = num_correct/labels.size(0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7036, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding='longest',  \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding['labels'] = torch.tensor(labels_list) # 入力にlabelsを含める。\n",
    "encoding = { k: v for k, v in encoding.items() }\n",
    "\n",
    "output = bert_scml(**encoding)\n",
    "loss = output.loss # 損失\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-8\n",
    "# データのダウンロード\n",
    "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
    "# データの解凍\n",
    "!unzip chABSA-dataset.zip \n",
    "# 7-9\n",
    "data = json.load(open('chABSA-dataset/e00030_ann.json'))\n",
    "print( data['sentences'][0] )\n",
    "# 7-10\n",
    "category_id = {'negative':0, 'neutral':1 , 'positive':2}\n",
    "\n",
    "dataset = []\n",
    "for file in glob.glob('chABSA-dataset/*.json'):\n",
    "    data = json.load(open(file))\n",
    "    # 各データから文章（text）を抜き出し、ラベル（'labels'）を作成\n",
    "    for sentence in data['sentences']:\n",
    "        text = sentence['sentence'] \n",
    "        labels = [0,0,0]\n",
    "        for opinion in sentence['opinions']:\n",
    "            labels[category_id[opinion['polarity']]] = 1\n",
    "        sample = {'text': text, 'labels': labels}\n",
    "        dataset.append(sample)\n",
    "# 7-11\n",
    "print(dataset[0])\n",
    "# 7-12\n",
    "# トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for sample in dataset:\n",
    "    text = sample['text']\n",
    "    labels = sample['labels']\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    encoding['labels'] = labels\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) \n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "#　データセットからデータローダを作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)\n",
    "# 7-13\n",
    "class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "            model_name, num_labels=num_labels\n",
    "        ) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_scml(**batch)\n",
    "        scores = output.logits\n",
    "        labels_predicted = ( scores > 0 ).int()\n",
    "        num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "        accuracy = num_correct/scores.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=5,\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassificationMultiLabel_pl(\n",
    "    MODEL_NAME, \n",
    "    num_labels=3, \n",
    "    lr=1e-5\n",
    ")\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')\n",
    "# 7-14\n",
    "# 入力する文章\n",
    "text_list = [\n",
    "    \"今期は売り上げが順調に推移したが、株価は低迷の一途を辿っている。\",\n",
    "    \"昨年から黒字が減少した。\",\n",
    "    \"今日の飲み会は楽しかった。\"\n",
    "]\n",
    "\n",
    "# モデルのロード\n",
    "best_model_path = checkpoint.best_model_path\n",
    "model = BertForSequenceClassificationMultiLabel_pl.load_from_checkpoint(best_model_path)\n",
    "bert_scml = model.bert_scml.cuda()\n",
    "\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding = 'longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# BERTへデータを入力し分類スコアを得る。\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits\n",
    "labels_predicted = ( scores > 0 ).int().cpu().numpy().tolist()\n",
    "\n",
    "# 結果を表示\n",
    "for text, label in zip(text_list, labels_predicted):\n",
    "    print('--')\n",
    "    print(f'入力：{text}')\n",
    "    print(f'出力：{label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob  import json  from tqdm import tqdm  import torch  from torch.utils.data import DataLoader  from transformers import BertJapaneseTokenizer, BertModel  import pytorch_lightning as pl  #日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 11, 616, 3290, 318, 13779], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, my dog is cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "config  = BertConfig()\n",
    "model = BertModel(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
