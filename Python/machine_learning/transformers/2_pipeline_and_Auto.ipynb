{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPn6Lyzv0O+6VX5zUIqwwiE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 環境設定"],"metadata":{"id":"UW-zOBKz_TCl"}},{"cell_type":"code","source":["import os\n","import sys\n","\n","def _is_in_kaggle() -> bool:\n","  \"\"\"Whether the current environment is in `Kaggle`.\"\"\"\n","  return str(_dh[0]) == '/kaggle/working'\n","\n","\n","def _is_in_colab() -> bool:\n","  \"\"\"Whether the current environment is in `Colab`.\"\"\"\n","  return 'google.colab' in str(get_ipython())"],"metadata":{"id":"BABU_Pa7jxxI","executionInfo":{"status":"ok","timestamp":1709343519576,"user_tz":-540,"elapsed":4,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["if _is_in_kaggle():\n","  print('in kaggle')\n","elif _is_in_colab():\n","  print('in colab')\n","  from google.colab import drive\n","  drive.mount('/content/drive')  # mount google drive\n","  sys.path.append('/content/drive/MyDrive/Colab Notebooks/my-modules')  # path from drive\n","else:\n","  print('in local')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Q-J4MNokEdx","executionInfo":{"status":"ok","timestamp":1709343521387,"user_tz":-540,"elapsed":1815,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}},"outputId":"1c06bf84-a4a3-4d32-c4cf-d6fd39d2446c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["in colab\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## pipeline\n","transformersの基本的な機能としてpipelineがある。\n","これは指定のモデルに対して、下記の一連の操作を自動実行してくれる機能である。\n","1. 入力を前処理\n","2. モデルに入力\n","3. 出力を後処理"],"metadata":{"id":"-t4KR1Y7ARVi"}},{"cell_type":"code","source":["from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","import torch\n","from IPython.display import display"],"metadata":{"id":"rgIzTHIj_G5u","executionInfo":{"status":"ok","timestamp":1709343538054,"user_tz":-540,"elapsed":396,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["classifier = pipeline(\"sentiment-analysis\")\n","sentences = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","    \"I do not dislike this.\",\n","]\n","classifier(sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAvg-suX_coW","executionInfo":{"status":"ok","timestamp":1709343544946,"user_tz":-540,"elapsed":797,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}},"outputId":"a0f5721c-e5fb-4acc-f7d7-76a91a7258fb"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455},\n"," {'label': 'POSITIVE', 'score': 0.9790695309638977}]"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["pipelineで自動実行される一連の処理に対して、それぞれtransformersでは個別にも機能が実装されており、AutoTokenizerとAutoModelがそれらに該当する。\n"],"metadata":{"id":"WbJ_oWB9BNnf"}},{"cell_type":"markdown","source":["## AutoTokenizer"],"metadata":{"id":"bDqOUWspdkIx"}},{"cell_type":"markdown","source":["まず、前処理については、AutoTokenizerを使う。\n","AutoTokenizerは、入力の前処理を自動的に行い、入力をトークンに分割し、それらをIDに変換する。\n","この処理はモデルごとに行われるため、AutoTokenizerではfrom_pretrainedでモデルを指定する必要がある。"],"metadata":{"id":"A5k4v_Glcgz0"}},{"cell_type":"code","source":["# モデルの指定とTokenizerの読み込み\n","ckpt = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(ckpt)\n","\n","raw_inputs = [\n","    \"Odapai is a man who has big tits.\",\n","    \"Satio made us very happy.\",\n","    \"We worked hard everyday at the dirty factory.\",\n","]\n","\n","# トークナイズ\n","inputs = tokenizer(\n","    raw_inputs,\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\"\n","    )\n","\n","display(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"id":"E_XF1Yt7B6GY","executionInfo":{"status":"ok","timestamp":1709343551035,"user_tz":-540,"elapsed":250,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}},"outputId":"7c656ee4-2a26-451a-dbd6-d8da2037dfd8"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["{'input_ids': tensor([[  101,  1051,  2850,  4502,  2072,  2003,  1037,  2158,  2040,  2038,\n","          1037,  2502, 25671,  1012,   102],\n","        [  101,  2938,  3695,  2081,  2149,  2200,  3407,  1012,   102,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  2057,  2499,  2524, 10126,  2012,  1996,  6530,  4713,  1012,\n","           102,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"]},"metadata":{}}]},{"cell_type":"markdown","source":["## AutoModel"],"metadata":{"id":"rxJ-O7O-dhas"}},{"cell_type":"markdown","source":["モデルの読み込みとモデルへの入力については、AutoModelを使う。\n","このときの出力は、AutoModelの最終層の出力と同様の形式になり、下記の例ではhidden_size: 768であり、この最終層出力が入力文章の分散表現となる。"],"metadata":{"id":"SIb6RI6QlWwj"}},{"cell_type":"code","source":["# モデルの指定とAutoModelの読み込み\n","model = AutoModel.from_pretrained(ckpt)  # AutoTokenizer と AutoModel は同様のモデルを指定する\n","\n","# モデルに入力\n","outputs = model(**inputs)\n","\n","# 出力\n","## last_hidden_state: 最終層の出力\n","## サイズが大きいのでshapeのみ確認する([batch_size, sequence_length, hidden_size])\n","display(f'outputs.last_hidden_state.shape: {outputs.last_hidden_state.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"VfvGZiaYSGTQ","executionInfo":{"status":"ok","timestamp":1709343555054,"user_tz":-540,"elapsed":262,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}},"outputId":"72904295-b1ea-486e-daca-67b97d510863"},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["'outputs.last_hidden_state.shape: torch.Size([3, 15, 768])'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}]},{"cell_type":"markdown","source":["このモデルの最終層出力は、各タスクに対応するような追加の層を与える事で個別のタスクを解くことができるようになる。\n","そのための機能の一つが、AutoModelForSequenceClassificationである。\n","AutoModelでは、入力文章の分散表現の獲得までを行なったが、AutoModelForSequenceClassificationは分散表現からラベルを予測するまでを一貫して行う。"],"metadata":{"id":"03SkxnTbpuCs"}},{"cell_type":"code","source":["# AutoModelForSequenceClassificationの読み込み\n","model = AutoModelForSequenceClassification.from_pretrained(ckpt)\n","\n","# モデルに入力\n","outputs = model(**inputs)\n","\n","# 出力\n","print(f'outputs.logits: {outputs.logits}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsnnnBAti_Cy","executionInfo":{"status":"ok","timestamp":1709343558572,"user_tz":-540,"elapsed":395,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}},"outputId":"6ef0402c-d29d-4f4f-c9ce-32ded3629128"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["outputs.logits: tensor([[-0.6493,  0.7493],\n","        [-4.3593,  4.6947],\n","        [ 2.8191, -2.5292]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["生の出力(logits)では解釈ができないため、logitsをSoftmaxで確率に変換する。"],"metadata":{"id":"Gm-JxJUo1YA9"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# 推論\n","probs = F.softmax(outputs.logits, dim=-1)\n","print(f'probs: {probs}')"],"metadata":{"id":"lLvDP8j3pDWl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709343784847,"user_tz":-540,"elapsed":351,"user":{"displayName":"chinchilla","userId":"17621678834685835832"}},"outputId":"3d8d39b2-f0a5-42af-9934-958a61e70c8d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["probs: tensor([[1.9802e-01, 8.0198e-01],\n","        [1.1692e-04, 9.9988e-01],\n","        [9.9527e-01, 4.7341e-03]], grad_fn=<SoftmaxBackward0>)\n","{0: 'NEGATIVE', 1: 'POSITIVE'}\n"]}]}]}